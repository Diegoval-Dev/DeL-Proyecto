Experimento,Modelo,F1 (dev),Accuracy,Precision,Recall,Latencia (ms)
Exp2: BERT Fine-tuning,bert-base-multilingual-cased,1.0,1.0,1.0,1.0,0.0
Exp3: Ensemble,Soft Voting (3 modelos),1.0,0.0,0.0,0.0,0.0
Exp2: BERT Fine-tuning,bert-base-spanish-wwm-cased,0.9876543209876543,0.9866666666666667,1.0,0.975609756097561,0.0
Exp1: Embeddings+LogReg,distiluse-base-multilingual-cased-v2,0.968421052631579,0.9651162790697675,0.9387755102040817,1.0,0.0010701112968977107
Exp1: Embeddings+LogReg,sentence_similarity_spanish_es,0.9387755102040817,0.9302325581395349,0.8846153846153846,1.0,0.001463779183321221
Exp1: Embeddings+LogReg,paraphrase-multilingual-MiniLM-L12-v2,0.9375,0.9302325581395349,0.9,0.9782608695652174,0.0013390252756517987
